---
title: "Overview statistic tests PB"  
author: "Jonas van Nijnatten"
date: "21 november 2018"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_float: true  
    toc_depth: 4
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{geometry}
- \geometry{a4paper, portrait, margin=.75in}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


contact: J.J.vanNijnatten@uva.nl

source code: https://github.com/jonasvannijnatten/R_Data_Visualization  

***

\newpage

#   R and package versions
## Versions  
software versions used for this tutorial:  
 - `r R.Version()$version.string`   
 - car-package version: `r packageVersion("car")` (`r packageDate("car")`)  
 - ez-package version:  `r packageVersion("ez")` (`r packageDate("ez")`)
 
##  Installation
Download & install the required packages:
```{r, eval = FALSE, echo=TRUE, results='hide', message=FALSE}
install.packages(pkgs="kableExtra",   repos="https://www.freestatistics.org/cran/")
install.packages(pkgs="car",   repos="https://www.freestatistics.org/cran/") 
install.packages(pkgs="ez",    repos="https://www.freestatistics.org/cran/")
```

Activate required packages:
```{r, eval=TRUE, message=FALSE}
library(package=kableExtra)
library(package=car)
library(package=ez)
```

***

\newpage

#  T-test

## Independent samples T-test
<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary>
```{r}
# generate data
N = 40
data.long = data.frame(
  ID = 1:N,
  condition = rep(x = c("A","B"), each = N),
  score = c(rnorm(n = N, mean = 25, sd = 6.5), rnorm(n = N, mean = 35, sd = 6.5))
)
```
</details>

```{r,echo=FALSE}
kable(data.long[c(1:5,41:45),], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

Test assumption of normality
```{r}
by(data = data.long$score, INDICES = data.long$condition, FUN = shapiro.test)
```
Test assumption of equality of variances
```{r}
leveneTest(y = data.long$score, group = data.long$condition)
```
**T-test**
```{r}
t.test(formula = score ~ condition, data = data.long, paired=FALSE, alternative="two.sided", var.equal=TRUE)
```

## Paired samples T-test
<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary>
```{r}
# generate data
N = 30
data.long = data.frame(
  ID = rep(1:N,2),
  condition = rep(x = c("A","B"), each = N),
  score = c(rnorm(n = N, mean = 25, sd = 6.5), rnorm(n = N, mean = 35, sd = 6.5))
)
```
</details>

```{r,echo=FALSE}
kable(data.long[c(1:5,31:35),], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

Test assumption of normality
```{r}
# calculate difference scores
diffScore = data.long$score[data.long$condition=="A"] - data.long$score[data.long$condition=="B"]
shapiro.test(diffScore)
```

**T-test**
```{r}
t.test(formula = score ~ condition, data = data.long, paired=TRUE, alternative="two.sided", var.equal=TRUE)
```

***
\newpage

#  Correlation

<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary> 
```{r}
# generate data
set.seed(05)
nrobs = 100
experience = rnorm(n = nrobs, mean = 15, sd = 3)
salary     = 10000 + ( 5 * experience ) + rnorm(n = nrobs, mean = 0, sd = 100)
data.long = data.frame(experience, salary)
# calculate correlation coefficient r
corr_coef = cor(x = data.long$experience, y = data.long$salary)
rm(list = c("nrobs", "experience", "salary"))
```
</details>   

```{r,echo=FALSE}
kable(data.long[1:5,], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```
Test assumption of normality
```{r}
apply(X = data.long, MARGIN = 2, FUN = shapiro.test)
```

**Pearson correlation**
```{r}
cor.test(x=data.long$experience, y=data.long$salary, alternative = "two.sided", method = "pearson")
```

**Spearman correlation**
```{r}
cor.test(x=data.long$experience, y=data.long$salary, alternative = "two.sided", method = "spearman")
```
 ***
 
\newpage 

# Regression

<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary> 
```{r}
# generate data
set.seed(05)
nrobs = 100
experience = rnorm(n = nrobs, mean = 15, sd = 3)
salary     = 10000 + ( 5 * experience ) + rnorm(n = nrobs, mean = 0, sd = 100)
data.long = data.frame(experience, salary)
# calculate correlation coefficient r
corr_coef = cor(x = data.long$experience, y = data.long$salary)
rm(list = c("nrobs", "experience", "salary"))
```
</details>   

```{r,echo=FALSE}
kable(data.long[1:5,], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

Fit linear model
```{r}
linearModel = lm(formula = salary ~ experience, data = data.long)
```

Test assumption of normality & equal variances
```{r}
# plot the residuals and look at the distribution around the 0-line and if the spread is equal over all x's.
plot(x=linearModel$fitted.values, y=linearModel$residuals, xlab = "predicted", ylab = "residuals"); abline(h=0)
shapiro.test(linearModel$residuals)
```
**Regression test**
```{r}
summary(linearModel)
```


***  
\newpage

# Logistic regression

<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary> 
```{r}
set.seed(01)
N = 30
data.long = data.frame(
  x = 1:N,
  y = round( 1 / (1 + exp(-.23*(1:N-10)))  +  # logistic function of x
               rnorm(n = N, mean = 0, sd = .3) # add some noise
             ) # round it to either 1 or 0
  )
```
</details>

```{r}
# fit a logistic model, which is a generalized linear model, hence glm() function
glm.fit <- glm(y ~ x, data = data.long, family = binomial('logit'))
# test the logistic model against the NULL model
anova(glm.fit, test="Chisq")
```
```{r}
plot(data.long$x, data.long$y) # plot the measured data
lines(formula = glm.fit$fitted.values ~ data.long$x) # plot the fitted model
```

***  
\newpage

#  One-way independent samples ANOVA
<details><summary><span style="color:dodgerBlue">Toon code om voorbeeld data te genereren </span></summary>
```{r, echo=TRUE}
set.seed(05)   # set seed
nrofconds = 3  # set number of conditions
nrofsubs  = 20 # set number of subjects
subj = as.factor(1:(nrofsubs*nrofconds))      # create array with subject IDs
condition = as.factor(rep(LETTERS[1:nrofconds],each=nrofsubs))   # create array with condition values
score = as.vector( replicate(
          nrofconds , rnorm(n = nrofsubs, mean = sample(8,1)+10 , sd = sample(5,1) ) 
        ) )                                     # create array with measurement values
data.long = data.frame(subj, condition, score);      # combine arrays into a data.frame
rm(list=c("subj", "condition","score","nrofconds","nrofsubs")) # delete unnecessary variables

```   
</details>  
   
```{r,echo=FALSE}
kable(data.long[c(1:2,21:22,41:42),], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

## Checking assumptions

Test assumption of normality
```{r}
by(data = data.long$score, INDICES = data.long$condition, FUN = shapiro.test)
```
Test equality of variances
```{r}
leveneTest(y = data.long$score, group = data.long$condition)
```

## Running the test

**ANOVA*** with the aov method
```{r}
aovModel = aov(formula = score ~ condition, data = data.long)
summary(aovModel)
```
**ANOVA*** with the linear model method
```{r}
lmModel = lm(formula = score ~ condition, data = data.long)
summary(lmModel)
```

## Post-hoc analysis

When using the aov method, use the Tukey post-hoc test
```{r}
TukeyHSD(aovModel)
```

When using the linear model method, use pairwise t-test
```{r}
pairwise.t.test(x = data.long$score, g = data.long$condition, paired = FALSE, p.adjust.method = "bonferroni")
```



***  
\newpage

#  Factorial independent samples ANOVA

<details><summary><span style="color:dodgerBlue">Show code for data generation</span></summary>
```{r, echo=TRUE}
set.seed(01)   # set seed
nrofcondsf1 = 3  # set number of conditions for factor 1
nrofcondsf2 = 2  # set number of conditions for factor 2
nrofsubs    = nrofcondsf1*nrofcondsf2*30 # set number of subjects per condition
subj = as.factor(1:(nrofsubs))      # create array with subject IDs
# create array witht treatment conditions
treatment = as.factor(rep(LETTERS[1:nrofcondsf1],each=nrofsubs/nrofcondsf1))   
# create array with control / experimental
control   = as.factor(rep(c("control","experimental"),times=nrofsubs/nrofcondsf2))   
# create array with measurement values
score = as.vector( replicate(nrofcondsf1, replicate ( 
          nrofcondsf2 , rnorm(
            n = (nrofsubs/(nrofcondsf1*nrofcondsf2)), 
            mean = 0 , sd = sample(5,1) ) + sample(8,1)+10
        ) ) )                             
# combine arrays into a data.frame
data.long = data.frame(subj, score, treatment, control);      
# delete unnecessary arrays
rm(list=c("control","nrofcondsf1","nrofcondsf2","nrofsubs","score","subj","treatment"))

```   
</details>     

```{r,echo=FALSE}
kable(data.long[c(1:2,61:62,121:122),], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

## Fitting the model

Fit model with multiple prediction factors
```{r}
model = lm(formula = score ~ treatment+control, data = data.long)
```

## Checking assumptions

Test assumption of normality
```{r, eval=FALSE}
hist(model$residuals)
plot(x = fitted(model), y = residuals(model), xlab="predicted", ylab="residuals"); abline(h=0)
by(data = data.long$score, INDICES = paste(data.long$treatment, data.long$control), FUN = shapiro.test)
```
<details><summary><span style="color:dodgerBlue">Show residuals histogram</span></summary>
```{r, echo=FALSE}
hist(model$residuals)
```
</details>
<details><summary><span style="color:dodgerBlue">Show residuals vs predicted plot</span></summary>
```{r, echo=FALSE}
plot(x = fitted(model), y = residuals(model), xlab="predicted", ylab="residuals"); abline(h=0)
```
</details>
<details><summary><span style="color:dodgerBlue">Show output of Shapiro-Wilk test</span></summary>
```{r, echo=FALSE}
by(data = data.long$score, INDICES = paste(data.long$treatment, data.long$control), FUN = shapiro.test)
```
</details>

Test equality of variances
```{r}
leveneTest(y = data.long$score, group = as.factor(paste(data.long$treatment, data.long$control)))
```

## Running the test

ANOVA with the linear model method
```{r}
model = lm(formula = score ~ treatment*control, data = data.long)
library(car)
Anova(mod = model, type = 'II')
summary(model)
```
\newpage 

## Post-hoc analysis

When there are one or more significant main-effects but no significant interaction effect post-hoc comparisons can be made for each factor separately using the pairwise.t.test() function.

Post-hoc comparison for the first factor:
```{r}
pairwise.t.test(
  x = data.long$score               # column name of the dependent variable
  ,g = data.long$treatment          # column name of the first within-subjects variable
  , paired = FALSE                   # since it is a paired design, set paired=TRUE
  , p.adjust.method = "bonferroni"  # use bonferroni corrected p-values
  )
```

Post-hoc comparison for the first factor:
```{r}
pairwise.t.test(
  x = data.long$score               # column name of the dependent variable
  ,g = data.long$control           # column name of the second within-subjects variable
  , paired = FALSE                   # since it is a paired design, set paired=TRUE
  , p.adjust.method = "bonferroni"  # use bonferroni corrected p-values
  )
```

When the interaction effect turns out significant, pairwise comparisons can be made between all combinations of conditions.

Post-hoc comparison for the interaction effect:
```{r}
pairwise.t.test(
  x = data.long$score               # column name of the dependent variable
  # merge both factor columns into a combined factor
  ,g = paste(data.long$treatment, data.long$control)           
  , paired = FALSE                   # since it is a paired design, set paired=TRUE
  , p.adjust.method = "bonferroni"  # use bonferroni corrected p-values
  )
```


***  
\newpage  

#  One-way repeated measures ANOVA
<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary>
```{r, echo=TRUE}
# Generate dataset
set.seed(01)   # set seed
nrofsubs  = 20 # set number of subjects
data.wide = data.frame(
  subj = as.factor(1:nrofsubs)   ,
  A =  rnorm(n = nrofsubs, mean = sample(8,1)+10 , sd = sample(5,1)) ,
  B =  rnorm(n = nrofsubs, mean = sample(8,1)+10 , sd = sample(5,1)) ,
  C =  rnorm(n = nrofsubs, mean = sample(8,1)+10 , sd = sample(5,1))
)
rm(list=c("nrofsubs")) # delete arrays
```
</details>  

```{r,echo=FALSE}
kable(data.wide[1:4,], row.names = FALSE, caption = 'data.wide') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

## Fitting the model

fit linear model
```{r}
model = lm(formula = cbind(data.wide$A, data.wide$B, data.wide$C)~1)
```  

## Checking assumptions

Test assumption of normality
```{r, results='hide', eval=FALSE}
shapiro.test(model$residuals)
hist(model$residuals)
```

```{r, echo=FALSE}
shapiro.test(model$residuals)
hist(model$residuals)
```

Test assumption of sphericity
```{r}
mauchly.test(model, X=~1)
```

## Running the test

Test the model using ANOVA
```{r}
anova(model, X = ~1, test="Spherical")
```

## Post-hoc analysis
For post-hoc analysis the data has to be transformed to the **long format**
<details><summary><span style="color:dodgerBlue">Show code for reshaping </span></summary>
```{r, results='hide'}
# reshape into wide format
data.long = reshape(data = data.wide
                   , direction = 'long'
                   , varying = c("A","B","C")
                   , timevar = 'treatment'
                   , times = c("A","B","C")
                   , v.names = c("score")
                   , idvar = "subj"
                   )
```
</details>

Use pairwise T-test to compare each combination of conditions
```{r}
pairwise.t.test(
  x = data.long$score               # column name of the dependent variable
  ,g = data.long$treatment          # column name of the first within-subjects variable
  , paired = TRUE                   # since it is a paired design, set paired=TRUE
  , p.adjust.method = "bonferroni"  # use bonferroni corrected p-values
  )
```

***  

\newpage

#  Factorial repeated measures ANOVA

<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary>
```{r, echo=TRUE}
set.seed(02)  # set seed
nrofsubs    = 20 # set number of subjects 

data.wide = data.frame(
  ID=1:nrofsubs,
  preA=rnorm(n=nrofsubs, mean = 09,sd = 3),
  preB=rnorm(n=nrofsubs, mean = 10,sd = 3),
  preC=rnorm(n=nrofsubs, mean = 11,sd = 3),
  postA=rnorm(n=nrofsubs, mean = 11,sd = 3),
  postB=rnorm(n=nrofsubs, mean = 15,sd = 3),
  postC=rnorm(n=nrofsubs, mean = 20,sd = 3)
)
rm(list="nrofsubs")
```
</details>   


```{r,echo=FALSE}
kable(data.wide[1:4,], row.names = FALSE, caption = 'data.wide') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

## Fitting the model

Fit linear model to the data, not yet seperating both independent factors:
```{r}
model = lm(cbind(data.wide$preA,
                 data.wide$preB,
                 data.wide$preC,
                 data.wide$postA,
                 data.wide$postB,
                 data.wide$postC
                 )~1
           )
```
Make a factor matrix clarifying the conditions. Name the colomns as the factors.
Use the parts of the column names of the data.frame in the wide format as variable values in the matrix.
```{r}
factorMatrix = data.frame(
  time=as.factor(rep(c("pre","post"),each=3)),
  condition = as.factor(rep(c("A","B","C"),2))
)
```
fit linear model specifying the two independent variables using the factorMatrix
```{r}
library(car)
factorialAnova = Anova(mod=model, idata=factorMatrix, idesign= ~time*condition, type='III')
```

## Checking assumptions
```{r, eval=FALSE  , results='hide'}
shapiro.test(model$residuals)
hist(model$residuals)
```

```{r, echo=FALSE}
shapiro.test(model$residuals)
hist(model$residuals)
```


## Running the test
run the factorial ANOVA 
```{r}
summary(object = factorialAnova, multivariate=FALSE)
```

The Mauchly Test checks each factor on the assumption of sphericity.
If the Mauchly Test shows a significant result for any of the factors or interaction between factors then a corrected p=value should be used instead of the normal, uncorrected p-value. 

## Post-hoc analysis

For post-hoc analysis the data has to be transformed to the **long format**
<details><summary><span style="color:dodgerBlue">Show code for reshaping </span></summary>
```{r, results='hide'}
# reshape into wide format
data.mix = reshape(data = data.wide
                   , direction = 'long'
                   , varying = list(c('preA','preB','preC'),c('postA','postB','postC'))
                   , timevar = 'treatment'
                   , times = c("A","B","C")
                   , v.names = c("pre","post")
                   , idvar = "ID"
                   )
data.long = reshape(data = data.mix
                   , direction = 'long'
                   , varying = c("pre","post")
                   , timevar = 'pre_post'
                   , times = c("pre","post")
                   , v.names = "score"
                   , idvar = c("ID", "treatment")
                   )
```
</details>

When there are one or more significant main-effects but no significant interaction effect post-hoc comparisons can be made for each factor separately using the pairwise.t.test() function.

Post-hoc comparison for the first factor:
```{r}
pairwise.t.test(
  x = data.long$score               # column name of the dependent variable
  ,g = data.long$treatment          # column name of the first within-subjects variable
  , paired = TRUE                   # since it is a paired design, set paired=TRUE
  , p.adjust.method = "bonferroni"  # use bonferroni corrected p-values
  )
```

Post-hoc comparison for the first factor:
```{r}
pairwise.t.test(
  x = data.long$score               # column name of the dependent variable
  ,g = data.long$pre_post           # column name of the second within-subjects variable
  , paired = TRUE                   # since it is a paired design, set paired=TRUE
  , p.adjust.method = "bonferroni"  # use bonferroni corrected p-values
  )
```

When the interaction effect turns out significant, pairwise comparisons can be made between all combinations of conditions.

Post-hoc comparison for the interaction effect:
```{r}
pairwise.t.test(
  x = data.long$score               # column name of the dependent variable
  # merge both factor columns into a combined factor
  ,g = paste(data.long$treatment, data.long$pre_post)           
  , paired = TRUE                   # since it is a paired design, set paired=TRUE
  , p.adjust.method = "bonferroni"  # use bonferroni corrected p-values
  )
```

***  

\newpage

# Mixed-Design ANOVA

<details><summary><span style="color:dodgerBlue">Show code for data generation</span></summary>
```{r, echo=TRUE}
set.seed(02)   # set seed
nrofcondsf1 = 3  # set number of conditions for factor 1
nrofcondsf2 = 2  # set number of conditions for factor 2
nrofsubs    = 30 # set number of subjects per condition
subj = as.factor(rep(x= 1:(nrofsubs*nrofcondsf2), times=nrofcondsf1))       # create array with subject IDs
# create array witht treatment conditions
treatment = as.factor(rep(LETTERS[1:nrofcondsf1],each=nrofsubs*nrofcondsf2))   
# create array with control / experimental
gender   = as.factor(rep(c("men","women"),times=nrofsubs*nrofcondsf1))   
# create array with measurement values
score = as.vector( replicate(nrofcondsf1, replicate ( 
          nrofcondsf2 , rnorm(
            n = (nrofsubs), 
            mean = 0 , sd = sample(5,1) ) + sample(8,1)+10
        ) ) )                             
# combine arrays into a data.frame
data.long = data.frame(subj, score, treatment, gender);      
# delete unnecessary arrays
rm(list=c("gender","nrofcondsf1","nrofcondsf2","nrofsubs","score","subj","treatment"))

```   
</details>     

```{r,echo=FALSE}
kable(data.long[c(1:2,61:62,121:122),], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

## Fitting the model

Fit mixed-model using ezANOVA() function
```{r}
mix.model = ezANOVA(data = data.long      # name of the data frame (long format)
                    , dv = score          # column name of the dependent variabele
                    , wid = subj          # column name indicating subject ID
                    , within = treatment  # column name of the within factor
                    , between = gender    # column name of the between factor
                    , return_aov = TRUE   # add information to the output for normality testing
                    )
```

## Checking assumptions

Test assumption of normality of the residuals
```{r, echo=TRUE, eval=FALSE, results='hide'}
shapiro.test(mix.model$aov$`subj:treatment`$residuals)

hist(mix.model$aov$`subj:treatment`$residuals
     , xlab = 'Residuals'
     , main = 'Histogram of residuals')
```
```{r, echo=FALSE, eval=TRUE}
shapiro.test(mix.model$aov$`subj:treatment`$residuals)
hist(mix.model$aov$`subj:treatment`$residuals, xlab = 'Residuals', main = 'Histogram of residuals')
```

Test assumption of sphericity
```{r}
mix.model$`Mauchly's Test for Sphericity`
```

## Running the test
If the assumption of sphericity is met use the normal, uncorrected ANOVA output
```{r}
mix.model$ANOVA
```

If the assumption of sphericity is violated, use the corrected ANOVA output
(The Greenhouse-Geisser correction is more conservative than the Huynd-Feldt correction)
```{r}
mix.model$`Sphericity Corrections`
```

## Post-hoc analysis

When there are one or more significant main-effects but no significant interaction effect post-hoc comparisons can be made using the pairwise.t.test() function.

Post-hoc comparison for the between-subjects factor:
```{r}
pairwise.t.test(
  x=data.long$score               # column name of the dependent variable
, g=data.long$gender              # column name of the independent between-subjects variable
, paired = TRUE                   # for the within factor, set paired=TRUE
, p.adjust.method = 'bonferroni'  # use bonferroni corrected p-values
)
```

Post-hoc comparison for the within-subjects factor:
```{r}
pairwise.t.test(
  x=data.long$score               # column name of the dependent variable
, g=data.long$treatment           # column name of the independent within-subjects variable
, paired = TRUE                   # for the within factor, set paired=TRUE
, p.adjust.method = 'bonferroni'  # use bonferroni corrected p-values
)
```

When the interaction-effect turns out significant, pairwise comparisons within each factor are not very informative about what the interaction looks like. Pairwise comparisons between all combinations of conditions is inappropriate when dealing with mixed designs. The best way to interpret an interaction effect is by looking at the interaction pattern in the data using descriptive statistics and visualization.

